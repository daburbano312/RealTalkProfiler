 
=== C:\Users\USUARIO\Documents\ProyectoFinalCA\Vokaturi.py === 
# Vokaturi.py
# Copyright (C) 2016,2018,2022 Paul Boersma, Johnny Ip, Toni Gojani
# version 2022-08-23

# This file is the Python interface to the Vokaturi library.
# The declarations are parallel to those in Vokaturi.h.

import ctypes

class Quality(ctypes.Structure):
	_fields_ = [
		("valid",                ctypes.c_int),
		("num_frames_analyzed",  ctypes.c_int),
		("num_frames_lost",      ctypes.c_int)]

class EmotionProbabilities(ctypes.Structure):
	_fields_ = [
		("neutrality",  ctypes.c_double),
		("happiness",   ctypes.c_double),
		("sadness",     ctypes.c_double),
		("anger",       ctypes.c_double),
		("fear",        ctypes.c_double)]

_library = None

def load(path_to_Vokaturi_library):
	global _library

	_library = ctypes.CDLL(path_to_Vokaturi_library)

	_library.VokaturiVoice_create.restype = ctypes.c_void_p
	_library.VokaturiVoice_create.argtypes = [
		ctypes.c_double,                           # sample_rate
		ctypes.c_int,                              # buffer_length
		ctypes.c_int]                              # multiThreading

	_library.VokaturiVoice_setRelativePriorProbabilities.restype = None
	_library.VokaturiVoice_setRelativePriorProbabilities.argtypes = [
		ctypes.c_void_p,                           # voice
		ctypes.POINTER (EmotionProbabilities)]     # priorEmotionProbabilities

	_library.VokaturiVoice_fill_float64array.restype = None
	_library.VokaturiVoice_fill_float64array.argtypes = [
		ctypes.c_void_p,                           # voice
		ctypes.c_int,                              # num_samples
		ctypes.POINTER (ctypes.c_double)]          # samples

	_library.VokaturiVoice_fill_float32array.restype = None
	_library.VokaturiVoice_fill_float32array.argtypes = [
		ctypes.c_void_p,                           # voice
		ctypes.c_int,                              # num_samples
		ctypes.POINTER (ctypes.c_float)]           # samples

	_library.VokaturiVoice_fill_int32array.restype = None
	_library.VokaturiVoice_fill_int32array.argtypes = [
		ctypes.c_void_p,                           # voice
		ctypes.c_int,                              # num_samples
		ctypes.POINTER (ctypes.c_int)]             # samples

	_library.VokaturiVoice_fill_int16array.restype = None
	_library.VokaturiVoice_fill_int16array.argtypes = [
		ctypes.c_void_p,                           # voice
		ctypes.c_int,                              # num_samples
		ctypes.POINTER (ctypes.c_short)]           # samples

	_library.VokaturiVoice_fill_float64value.restype = None
	_library.VokaturiVoice_fill_float64value.argtypes = [
		ctypes.c_void_p,                           # voice
		ctypes.c_double]                           # sample

	_library.VokaturiVoice_fill_float32value.restype = None
	_library.VokaturiVoice_fill_float32value.argtypes = [
		ctypes.c_void_p,                           # voice
		ctypes.c_float]                            # sample

	_library.VokaturiVoice_fill_int32value.restype = None
	_library.VokaturiVoice_fill_int32value.argtypes = [
		ctypes.c_void_p,                           # voice
		ctypes.c_int]                              # sample

	_library.VokaturiVoice_fill_int16value.restype = None
	_library.VokaturiVoice_fill_int16value.argtypes = [
		ctypes.c_void_p,                           # voice
		ctypes.c_int]                              # sample (yes, 32 bits, because of C argument sizes)

	_library.VokaturiVoice_fillInterlacedStereo_float64array.restype = None
	_library.VokaturiVoice_fillInterlacedStereo_float64array.argtypes = [
		ctypes.c_void_p,                           # voice left-channel
		ctypes.c_void_p,                           # voice right-channel
		ctypes.c_int,                              # num_samples_per_channel
		ctypes.POINTER (ctypes.c_double)]          # samples

	_library.VokaturiVoice_fillInterlacedStereo_float32array.restype = None
	_library.VokaturiVoice_fillInterlacedStereo_float32array.argtypes = [
		ctypes.c_void_p,                           # voice left-channel
		ctypes.c_void_p,                           # voice right-channel
		ctypes.c_int,                              # num_samples_per_channel
		ctypes.POINTER (ctypes.c_float)]           # samples

	_library.VokaturiVoice_fillInterlacedStereo_int32array.restype = None
	_library.VokaturiVoice_fillInterlacedStereo_int32array.argtypes = [
		ctypes.c_void_p,                           # voice left-channel
		ctypes.c_void_p,                           # voice right-channel
		ctypes.c_int,                              # num_samples_per_channel
		ctypes.POINTER (ctypes.c_int)]             # samples

	_library.VokaturiVoice_fillInterlacedStereo_int16array.restype = None
	_library.VokaturiVoice_fillInterlacedStereo_int16array.argtypes = [
		ctypes.c_void_p,                           # voice left-channel
		ctypes.c_void_p,                           # voice right-channel
		ctypes.c_int,                              # num_samples_per_channel
		ctypes.POINTER (ctypes.c_short)]           # samples

	_library.VokaturiVoice_extract.restype = None
	_library.VokaturiVoice_extract.argtypes = [
		ctypes.c_void_p,                           # voice
		ctypes.POINTER (Quality),                  # quality
		ctypes.POINTER (EmotionProbabilities)]     # emotionProbabilities

	_library.VokaturiVoice_reset.restype = None
	_library.VokaturiVoice_reset.argtypes = [
		ctypes.c_void_p]                           # voice

	_library.VokaturiVoice_destroy.restype = None
	_library.VokaturiVoice_destroy.argtypes = [
		ctypes.c_void_p]                           # voice

	_library.Vokaturi_versionAndLicense.restype = ctypes.c_char_p
	_library.Vokaturi_versionAndLicense.argtypes = []

class Voice:

	def __init__(self, sample_rate, buffer_length, multi_threading):
		self._voice = _library.VokaturiVoice_create(sample_rate, buffer_length, multi_threading)

	def setRelativePriorProbabilities(self, priorEmotionProbabilities):
		_library.VokaturiVoice_setRelativePriorProbabilities(self._voice, priorEmotionProbabilities)

	def fill_float64array(self, num_samples, samples):
		_library.VokaturiVoice_fill_float64array(self._voice, num_samples, samples)

	def fill_float32array(self, num_samples, samples):
		_library.VokaturiVoice_fill_float32array(self._voice, num_samples, samples)

	def fill_int32array(self, num_samples, samples):
		_library.VokaturiVoice_fill_int32array(self._voice, num_samples, samples)

	def fill_int16array(self, num_samples, samples):
		_library.VokaturiVoice_fill_int16array(self._voice, num_samples, samples)

	def fill_float64value(self, sample):
		_library.VokaturiVoice_fill_float64value(self._voice, sample)

	def fill_float32value(self, sample):
		_library.VokaturiVoice_fill_float32value(self._voice, sample)

	def fill_int32value(self, sample):
		_library.VokaturiVoice_fill_int32value(self._voice, sample)

	def fill_int16value(self, sample):
		_library.VokaturiVoice_fill_int16value(self._voice, sample)

	def extract(self, quality, emotionProbabilities):
		_library.VokaturiVoice_extract(self._voice, quality, emotionProbabilities)

	def reset(self):
		_library.VokaturiVoice_reset(self._voice)

	def destroy(self):
		if not _library is None:
			_library.VokaturiVoice_destroy(self._voice)

def Voices_fillInterlacedStereo_float64array(left, right, num_samples_per_channel, samples):
	_library.VokaturiVoice_fillInterlacedStereo_float64array(left._voice, right._voice, num_samples_per_channel, samples)

def Voices_fillInterlacedStereo_float32array(left, right, num_samples_per_channel, samples):
	_library.VokaturiVoice_fillInterlacedStereo_float32array(left._voice, right._voice, num_samples_per_channel, samples)

def Voices_fillInterlacedStereo_int32array(left, right, num_samples_per_channel, samples):
	_library.VokaturiVoice_fillInterlacedStereo_int32array(left._voice, right._voice, num_samples_per_channel, samples)

def Voices_fillInterlacedStereo_int16array(left, right, num_samples_per_channel, samples):
	_library.VokaturiVoice_fillInterlacedStereo_int16array(left._voice, right._voice, num_samples_per_channel, samples)

def versionAndLicense():
	return _library.Vokaturi_versionAndLicense().decode("UTF-8")

def float64array(size):
	return (ctypes.c_double * size)()

def float32array(size):
	return (ctypes.c_float * size)()

def int32array(size):
	return (ctypes.c_int * size)()

def int16array(size):
	return (ctypes.c_short * size)()

 
=== C:\Users\USUARIO\Documents\ProyectoFinalCA\config\settings.py === 
# config/settings.py
import os
from pathlib import Path
from dotenv import load_dotenv
from typing import Dict, Any

# Cargar variables de entorno desde .env
load_dotenv()

class Config:
    """Configuración base para todos los entornos"""
    
    # Directorios principales
    BASE_DIR: Path = Path(__file__).resolve().parent.parent
    MODEL_DIR: Path = BASE_DIR / "models"
    LIB_DIR: Path = BASE_DIR / "libs"
    
    # Configuración de audio
    AUDIO_SAMPLE_RATE: int = 16000
    AUDIO_CHUNK_SIZE: int = 2048
    MAX_AUDIO_DURATION: int = 300  # segundos
    
    # Configuración de Vokaturi
    VOKATURI_DLL_PATH: Path = LIB_DIR / "OpenVokaturi-4-0-win64.dll"
    
    # Configuración de Vosk
    VOSK_MODEL_PATH: Path = MODEL_DIR / "vosk" / "es"
    
    # OpenAI
    OPENAI_API_KEY: str = os.getenv("OPENAI_API_KEY", "")
    OPENAI_MODEL: str = "gpt-4-turbo"
    OPENAI_TEMPERATURE: float = 0.7
    
    # Configuración de la API
    API_HOST: str = "0.0.0.0"
    API_PORT: int = 5000
    API_DEBUG: bool = False
    API_MAX_CONTENT_LENGTH: int = 16 * 1024 * 1024  # 16MB
    
    # Configuración de logging
    LOG_LEVEL: str = "INFO"
    LOG_FORMAT: str = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    
    @classmethod
    def validate(cls):
        """Validar configuraciones esenciales"""
        errors = []
        
        if not cls.VOKATURI_DLL_PATH.exists():
            errors.append(f"Vokaturi DLL no encontrada en {cls.VOKATURI_DLL_PATH}")
            
        if not cls.VOSK_MODEL_PATH.exists():
            errors.append(f"Modelo Vosk no encontrado en {cls.VOSK_MODEL_PATH}")
            
        if not cls.OPENAI_API_KEY:
            errors.append("OPENAI_API_KEY no configurada")
            
        if errors:
            raise EnvironmentError("\n".join(errors))
    
    @classmethod
    def as_dict(cls) -> Dict[str, Any]:
        """Devolver configuración como diccionario"""
        return {
            key: value 
            for key, value in cls.__dict__.items()
            if not key.startswith("__") and not callable(value)
        }

class DevelopmentConfig(Config):
    """Configuración para entorno de desarrollo"""
    API_DEBUG: bool = True
    LOG_LEVEL: str = "DEBUG"

class ProductionConfig(Config):
    """Configuración para entorno de producción"""
    API_DEBUG: bool = False
    LOG_LEVEL: str = "WARNING"

class TestingConfig(Config):
    """Configuración para pruebas"""
    API_DEBUG: bool = True
    TESTING: bool = True
    OPENAI_MODEL: str = "gpt-3.5-turbo"

def get_settings(env: str = None) -> Config:
    """Factory para obtener configuración según entorno"""
    env = env or os.getenv("APP_ENV", "development")
    
    configs = {
        "development": DevelopmentConfig,
        "production": ProductionConfig,
        "testing": TestingConfig
    }
    
    if env not in configs:
        raise ValueError(f"Entorno {env} no válido. Opciones: {', '.join(configs.keys())}")
    
    config = configs[env]
    config.validate()
    
    return config

# Configuración activa
settings = get_settings().
 
=== C:\Users\USUARIO\Documents\ProyectoFinalCA\core\entities\emotion.py === 
# core/entities/emotion.py
from dataclasses import dataclass
from enum import Enum

class EmotionType(Enum):
    NEUTRAL = "Neutral"
    HAPPY = "Feliz"
    ANGRY = "Enojado"
    UNIDENTIFIED = "NoIdentificado"

@dataclass(frozen=True)
class EmotionAnalysis:
    emotion_type: EmotionType
    neutrality_prob: float
    happiness_prob: float
    anger_prob: float
    unidentified_prob: float = 0.0

    def __post_init__(self):
        if not (0 <= self.neutrality_prob <= 1 and
                0 <= self.happiness_prob <= 1 and
                0 <= self.anger_prob <= 1 and
                0 <= self.unidentified_prob <= 1):
            raise ValueError("Las probabilidades deben estar entre 0 y 1")
            
        total = sum([self.neutrality_prob, 
                    self.happiness_prob, 
                    self.anger_prob,
                    self.unidentified_prob])
        
        if not 0.99 <= total <= 1.01:
            raise ValueError(f"Las probabilidades deben sumar 1 (actual: {total:.2f})")

    def to_dict(self):
        return {
            "emotion_type": self.emotion_type.value,
            "neutrality": self.neutrality_prob,
            "happiness": self.happiness_prob,
            "anger": self.anger_prob,
            "unidentified": self.unidentified_prob
        }.
 
=== C:\Users\USUARIO\Documents\ProyectoFinalCA\core\entities\suggestion.py === 
# core/entities/suggestion.py
from dataclasses import dataclass
from datetime import datetime
from .emotion import EmotionType

@dataclass(frozen=True)
class Suggestion:
    text: str
    emotion_type: EmotionType
    context: str
    priority: int = 1
    generated_at: datetime = datetime.now()

    def __post_init__(self):
        if len(self.text.strip()) == 0:
            raise ValueError("El texto de sugerencia no puede estar vacío")
        
        if self.priority < 1 or self.priority > 5:
            raise ValueError("La prioridad debe estar entre 1 y 5")

    def to_dict(self):
        return {
            "suggestion_text": self.text,
            "target_emotion": self.emotion_type.value,
            "context": self.context,
            "priority": self.priority,
            "generated_at": self.generated_at.isoformat()
        }

@dataclass(frozen=True)
class SuggestionHistory:
    suggestions: tuple[Suggestion, ...]
    
    def get_latest(self, n: int = 3) -> list[Suggestion]:
        return sorted(
            self.suggestions, 
            key=lambda x: x.generated_at, 
            reverse=True
        )[:n]

    def filter_by_emotion(self, emotion_type: EmotionType) -> list[Suggestion]:
        return [s for s in self.suggestions if s.emotion_type == emotion_type].
 
=== C:\Users\USUARIO\Documents\ProyectoFinalCA\core\entities\transcription.py === 
# core/entities/transcription.py
from dataclasses import dataclass
from datetime import datetime
from typing import Optional
from .emotion import EmotionAnalysis

@dataclass(frozen=True)
class Transcription:
    text: str
    emotion_analysis: EmotionAnalysis
    audio_duration: float
    timestamp: datetime = datetime.now()
    confidence: Optional[float] = None
    audio_source: Optional[str] = None

    def __post_init__(self):
        if len(self.text.strip()) == 0:
            raise ValueError("El texto de la transcripción no puede estar vacío")
        
        if self.audio_duration <= 0:
            raise ValueError("La duración del audio debe ser positiva")

    def to_dict(self):
        return {
            "text": self.text,
            "emotion": self.emotion_analysis.to_dict(),
            "audio_duration": self.audio_duration,
            "timestamp": self.timestamp.isoformat(),
            "confidence": self.confidence,
            "audio_source": self.audio_source
        }.
 
=== C:\Users\USUARIO\Documents\ProyectoFinalCA\core\entities\__init__.py === 
from .emotion import EmotionType, EmotionAnalysis
from .transcription import Transcription
from .suggestion import Suggestion.
 
=== C:\Users\USUARIO\Documents\ProyectoFinalCA\core\use_cases\audio_processing.py === 
# core/use_cases/audio_processing.py
import logging
from typing import List, Tuple
from ..entities import Transcription
from interfaces.audio import SpeechToText

class AudioProcessingUseCase:
    def __init__(self, stt_service: SpeechToText):
        self.stt_service = stt_service
        self.logger = logging.getLogger(__name__)
    
    def process_audio(self, audio_data: bytes) -> Transcription:
        """Procesa audio completo y devuelve transcripción con metadatos"""
        try:
            result = self.stt_service.transcribe(audio_data)
            return self._create_transcription_object(result)
        except Exception as e:
            self.logger.error(f"Error en procesamiento de audio: {str(e)}")
            raise AudioProcessingError("Falló el procesamiento de audio") from e
    
    def process_streaming_audio(self, audio_chunks: List[bytes]) -> Tuple[List[Transcription], float]:
        """Procesa audio en streaming con gestión de contexto"""
        transcriptions = []
        total_duration = 0.0
        
        for chunk in audio_chunks:
            transcription = self.stt_service.transcribe(chunk)
            total_duration += transcription.audio_duration
            transcriptions.append(transcription)
        
        return transcriptions, total_duration
    
    def _create_transcription_object(self, raw_result: dict) -> Transcription:
        """Factory method para crear objeto Transcription validado"""
        return Transcription(
            text=raw_result['text'],
            emotion_analysis=raw_result.get('emotion_analysis'),
            audio_duration=raw_result['duration'],
            confidence=raw_result.get('confidence', 0.0),
            audio_source=raw_result.get('source')
        )

class AudioProcessingError(Exception):
    """Error personalizado para fallos en procesamiento de audio""".
 
=== C:\Users\USUARIO\Documents\ProyectoFinalCA\core\use_cases\emotion_analysis.py === 
# core/use_cases/emotion_analysis.py
from typing import Tuple, Optional
from ..entities import EmotionAnalysis, Transcription
from interfaces.emotion_detection import EmotionDetector

class EmotionAnalysisUseCase:
    def __init__(self, detector: EmotionDetector):
        self.detector = detector
        self._validation_threshold = 0.85  # Umbral de confianza mínimo
    
    def analyze_audio(self, audio_data: bytes) -> EmotionAnalysis:
        """
        Analiza un fragmento de audio y devuelve el análisis de emociones
        con validación de calidad del resultado
        """
        if not audio_data:
            raise ValueError("Datos de audio no pueden estar vacíos")
        
        raw_analysis = self.detector.analyze(audio_data)
        validated_analysis = self._validate_analysis(raw_analysis)
        
        return validated_analysis
    
    def analyze_transcription(self, transcription: Transcription) -> EmotionAnalysis:
        """
        Realiza análisis de emociones a partir de una transcripción existente
        aplicando reglas de negocio específicas
        """
        if transcription.confidence and transcription.confidence < 0.7:
            raise LowConfidenceError("La confianza de la transcripción es demasiado baja")
        
        return self.detector.analyze_text(transcription.text)
    
    def _validate_analysis(self, analysis: EmotionAnalysis) -> EmotionAnalysis:
        """Aplica reglas de validación de calidad del análisis"""
        total = sum([
            analysis.neutrality_prob,
            analysis.happiness_prob,
            analysis.anger_prob,
            analysis.unidentified_prob
        ])
        
        if abs(total - 1.0) > 0.01:
            raise InvalidAnalysisError("Las probabilidades no suman 1")
        
        if max([
            analysis.neutrality_prob,
            analysis.happiness_prob,
            analysis.anger_prob
        ]) < self._validation_threshold:
            analysis = analysis._replace(
                unidentified_prob=1 - sum([
                    analysis.neutrality_prob,
                    analysis.happiness_prob,
                    analysis.anger_prob
                ])
            )
        
        return analysis

class LowConfidenceError(Exception):
    """Error personalizado para confianza baja en transcripción"""

class InvalidAnalysisError(Exception):
    """Error para análisis que no cumple requisitos de calidad""".
 
=== C:\Users\USUARIO\Documents\ProyectoFinalCA\core\use_cases\suggestion_generation.py === 
# core/use_cases/suggestion_generation.py
from typing import List
from ..entities import Suggestion, EmotionAnalysis, Transcription
from interfaces.ai import SuggestionGenerator

class SuggestionGenerationUseCase:
    def __init__(self, generators: List[SuggestionGenerator]):
        self.generators = generators
        self._priority_rules = {
            'Enojado': 5,
            'Feliz': 3,
            'Neutral': 2,
            'NoIdentificado': 1
        }
    
    def generate_from_emotion(self, analysis: EmotionAnalysis, context: str = "") -> List[Suggestion]:
        """Genera sugerencias basadas en análisis de emociones con priorización"""
        suggestions = []
        
        for generator in self.generators:
            try:
                raw_suggestions = generator.generate(analysis, context)
                prioritized = self._apply_priority_rules(raw_suggestions)
                suggestions.extend(prioritized)
            except Exception as e:
                continue  # Fail silently for non-critical generators
        
        return sorted(suggestions, key=lambda x: x.priority, reverse=True)[:3]
    
    def generate_from_transcription(self, transcription: Transcription) -> List[Suggestion]:
        """Genera sugerencias contextuales basadas en transcripción completa"""
        context = self._extract_context(transcription.text)
        return self.generate_from_emotion(transcription.emotion_analysis, context)
    
    def _apply_priority_rules(self, suggestions: List[Suggestion]) -> List[Suggestion]:
        """Aplica reglas de negocio para priorización de sugerencias"""
        for suggestion in suggestions:
            new_priority = self._priority_rules.get(
                suggestion.emotion_type.value, 
                suggestion.priority
            )
            suggestion.priority = max(new_priority, suggestion.priority)
        
        return suggestions
    
    def _extract_context(self, text: str) -> str:
        """Extrae contexto clave del texto para generar sugerencias relevantes"""
        keywords = {'cuenta', 'pago', 'problema', 'información', 'servicio'}
        found = [word for word in text.lower().split() if word in keywords]
        return " ".join(found) if found else "general"

class SuggestionGenerationError(Exception):
    """Error personalizado para fallos en generación de sugerencias""".
 
=== C:\Users\USUARIO\Documents\ProyectoFinalCA\core\use_cases\__init__.py === 
from .audio_processing import AudioProcessingUseCase
from .emotion_analysis import EmotionAnalysisUseCase
from .suggestion_generation import SuggestionGenerationUseCase.
 
=== C:\Users\USUARIO\Documents\ProyectoFinalCA\infrastructure\ai\openai_generator.py === 
# infrastructure/ai/openai_generator.py
import openai
from datetime import datetime
from typing import List, Dict, Optional
import logging
from core.entities import EmotionAnalysis, Transcription, Suggestion, EmotionType
from interfaces.ai.suggestion_generator import (
    SuggestionGenerator,
    SuggestionType,
    SuggestionGenerationError,
    ModelOverloadError,
    InvalidInputError
)
from openai import OpenAI


class OpenAIGenerator(SuggestionGenerator):
    def __init__(self, api_key: str, default_model: str = "gpt-4-turbo"):
        super().__init__()
        self.client = OpenAI(api_key=api_key)
        self.default_model = default_model
        self._temperature = 0.7
        self._max_tokens = 300
        self._creativity = 0.5
        self._max_suggestions = 5
        self._request_count = 0
        
        # Plantillas de prompts en múltiples idiomas
        self.prompt_templates = {
            'es': {
                'emotion': (
                    "Eres un asistente para agentes de call center. Genera {num} sugerencias profesionales "
                    "basadas en la emoción detectada: {emotion} (confianza: {confidence:.0%}). "
                    "Contexto adicional: {context}. Formato: • [Tipo] Sugerencia corta"
                ),
                'text': (
                    "Analiza el siguiente mensaje del cliente y genera {num} sugerencias de acción "
                    "para el agente: '{text}'. Contexto: {context}. Formato: • [Tipo] Sugerencia"
                )
            },
            'en': {
                'emotion': (
                    "You're a call center assistant. Generate {num} professional suggestions "
                    "based on detected emotion: {emotion} (confidence: {confidence:.0%}). "
                    "Additional context: {context}. Format: • [Type] Short suggestion"
                )
            }
        }

    def generate_from_emotion(
        self, 
        emotion_analysis: EmotionAnalysis,
        context: Optional[Dict] = None
    ) -> List[Suggestion]:
        self._validate_inputs(emotion_analysis=emotion_analysis)
        context_str = self._parse_context(context)
        
        try:
            prompt = self._build_emotion_prompt(emotion_analysis, context_str)
            response = self._call_openai_api(prompt)
            return self._parse_suggestions(response, emotion_analysis.predominant_emotion)
        except openai.RateLimitError as e:
            raise ModelOverloadError("Límite de tasa excedido") from e
        except Exception as e:
            logging.error(f"OpenAI API error: {str(e)}")
            raise SuggestionGenerationError("Error generando sugerencias") from e

    def generate_from_text(
        self, 
        text: str,
        language: str = "es",
        context: Optional[Dict] = None
    ) -> List[Suggestion]:
        self._validate_inputs(text=text)
        context_str = self._parse_context(context)
        
        try:
            prompt = self._build_text_prompt(text, context_str, language)
            response = self._call_openai_api(prompt)
            return self._parse_suggestions(response, None)
        except openai.APIError as e:
            raise ModelOverloadError("Error en API de OpenAI") from e

    def generate_from_transcription(self, transcription: Transcription) -> List[Suggestion]:
        return self.generate_from_text(
            text=transcription.text,
            context={
                'emotion': transcription.emotion_analysis.predominant_emotion.value,
                'duration': transcription.audio_duration
            }
        )

    def get_supported_languages(self) -> List[str]:
        return ['es', 'en']

    def model_version(self) -> str:
        return self.default_model

    def get_max_suggestions(self) -> int:
        return self._max_suggestions

    def set_generation_parameters(
        self,
        temperature: float = 0.7,
        max_length: int = 300,
        creativity: float = 0.5
    ) -> None:
        self._temperature = max(0.1, min(temperature, 1.0))
        self._max_tokens = max(100, min(max_length, 1000))
        self._creativity = creativity

    def get_performance_metrics(self) -> Dict:
        return {
            'requests': self._request_count,
            'model': self.default_model,
            'parameters': {
                'temperature': self._temperature,
                'max_tokens': self._max_tokens
            }
        }

    def _build_emotion_prompt(self, analysis: EmotionAnalysis, context: str) -> str:
        template = self.prompt_templates['es']['emotion']
        return template.format(
            num=self._max_suggestions,
            emotion=analysis.predominant_emotion.value,
            confidence=analysis.confidence,
            context=context
        )

    def _build_text_prompt(self, text: str, context: str, language: str) -> str:
        template = self.prompt_templates.get(language, self.prompt_templates['es'])['text']
        return template.format(
            num=self._max_suggestions,
            text=text[:500],  # Limitar texto para evitar sobrecarga
            context=context
        )

    def _call_openai_api(self, prompt: str) -> str:
        self._request_count += 1
        response = self.client.chat.completions.create(
            model=self.default_model,
            messages=[
                {"role": "system", "content": "Eres un asistente especializado en servicio al cliente"},
                {"role": "user", "content": prompt}
            ],
            temperature=self._temperature,
            max_tokens=self._max_tokens,
            n=1
        )
        return response.choices[0].message.content

    def _parse_suggestions(self, raw_text: str, target_emotion: Optional[EmotionType]) -> List[Suggestion]:
        suggestions = []
        lines = [line.strip() for line in raw_text.split('\n') if line.strip()]
        
        for line in lines:
            try:
                suggestion_type, text = self._parse_line(line)
                suggestions.append(Suggestion(
                    text=text,
                    suggestion_type=suggestion_type,
                    target_emotion=target_emotion or EmotionType.UNIDENTIFIED,
                    confidence=self._creativity,
                    metadata={
                        'source': 'OpenAI',
                        'model': self.default_model
                    }
                ))
            except (ValueError, IndexError):
                continue
                
        return self._apply_business_rules(suggestions)

    def _parse_line(self, line: str) -> tuple[SuggestionType, str]:
        line = line.strip('•-* ')
        if ']' in line:
            type_part, text_part = line.split(']', 1)
            suggestion_type = type_part.strip(' []').upper()
            return (SuggestionType[suggestion_type], text_part.strip())
        return (SuggestionType.STANDARD, line)

    def _parse_context(self, context: Optional[Dict]) -> str:
        if not context:
            return "sin contexto adicional"
        return ", ".join(f"{k}: {v}" for k, v in context.items()).
 
=== C:\Users\USUARIO\Documents\ProyectoFinalCA\infrastructure\ai\__init__.py === 
from interfaces.emotion_detection.emotion_detector import EmotionType.
 
=== C:\Users\USUARIO\Documents\ProyectoFinalCA\infrastructure\audio\vokaturi_recorder.py === 
# infrastructure/audio/vokaturi_recorder.py
import Vokaturi
import pyaudio
import numpy as np
from typing import Generator, Tuple, Optional
from interfaces.audio.recorder import AudioRecorder, AudioFormat, RecordingError, DeviceConfigurationError

class VokaturiRecorder(AudioRecorder):
    def __init__(self, format: AudioFormat = AudioFormat(), dll_path: str = ""):
        super().__init__(format)
        self._buffer = Vokaturi.float32array(self.format.chunk_size)
        self._stream = None
        self._audio_data = bytearray()
        self._is_recording = False
        self._load_library(dll_path)
        
    def _load_library(self, dll_path: str):
        try:
            Vokaturi.load(dll_path)
            self._voice = Vokaturi.Voice(
                self.format.sample_rate,
                self.format.chunk_size,
                True
            )
        except Exception as e:
            raise DeviceConfigurationError(f"Error loading Vokaturi library: {str(e)}")

    def start_recording(self) -> None:
        if self._is_recording:
            return
            
        self._audio_data = bytearray()
        self._pa = pyaudio.PyAudio()
        
        try:
            self._stream = self._pa.open(
                rate=self.format.sample_rate,
                channels=self.format.channels,
                format=pyaudio.paFloat32,
                input=True,
                frames_per_buffer=self.format.chunk_size,
                stream_callback=self._callback
            )
            self._is_recording = True
        except Exception as e:
            raise RecordingError(f"Failed to start recording: {str(e)}")

    def stop_recording(self) -> None:
        if self._stream:
            self._stream.stop_stream()
            self._stream.close()
            self._pa.terminate()
            self._is_recording = False

    def _callback(self, in_data, frame_count, time_info, status):
        audio_array = np.frombuffer(in_data, dtype=np.float32)
        self._buffer[0:frame_count] = audio_array
        self._voice.fill_float32array(frame_count, self._buffer)
        self._audio_data.extend(in_data)
        return (in_data, pyaudio.paContinue)

    @property
    def is_recording(self) -> bool:
        return self._is_recording

    def get_audio_stream(self) -> Generator[bytes, None, None]:
        while self._is_recording:
            if len(self._audio_data) >= self.format.chunk_size:
                chunk = bytes(self._audio_data[:self.format.chunk_size])
                self._audio_data = self._audio_data[self.format.chunk_size:]
                yield chunk

    def get_full_recording(self) -> bytes:
        return bytes(self._audio_data)

    def list_devices(self) -> Tuple[int, str]:
        devices = []
        for i in range(self._pa.get_device_count()):
            info = self._pa.get_device_info_by_index(i)
            devices.append((i, info['name']))
        return tuple(devices)

    def configure_device(self, device_index: int) -> None:
        self.stop_recording()
        self.start_recording().
 
=== C:\Users\USUARIO\Documents\ProyectoFinalCA\infrastructure\audio\vosk_stt.py === 
# infrastructure/audio/vosk_stt.py
import vosk
import json
import logging
from typing import Generator, Tuple
from pathlib import Path
from interfaces.audio.speech_to_text import SpeechToText, TranscriptionConfig, TranscriptionResult
from core.entities.transcription import Transcription

class VoskSpeechToText(SpeechToText):
    def __init__(self, config: TranscriptionConfig = TranscriptionConfig()):
        super().__init__(config)
        self._model = None
        self._recognizer = None
        self._load_model()

    def _load_model(self):
        model_path = Path("models/vosk/es")
        if not model_path.exists():
            raise FileNotFoundError(f"Vosk model not found at {model_path}")
            
        try:
            self._model = vosk.Model(str(model_path))
            self._recognizer = vosk.KaldiRecognizer(self._model, self.config.sample_rate)
        except Exception as e:
            logging.error(f"Failed to load Vosk model: {str(e)}")
            raise

    def transcribe(self, audio_data: bytes) -> Transcription:
        self._validate_audio_input(audio_data)
        
        try:
            if self._recognizer.AcceptWaveform(audio_data):
                result = json.loads(self._recognizer.Result())
                return self._create_transcription(
                    result['text'],
                    result.get('conf', 0.0),
                    len(audio_data) / (2 * self.config.sample_rate)
                )
            return self._create_transcription("", 0.0, 0.0)
        except Exception as e:
            logging.error(f"Transcription failed: {str(e)}")
            return self._create_transcription("Error en transcripción", 0.0, 0.0)

    def transcribe_streaming(self, audio_stream: Generator[bytes, None, None]) -> Generator[TranscriptionResult, None, None]:
        for chunk in audio_stream:
            if self._recognizer.AcceptWaveform(chunk):
                result = json.loads(self._recognizer.Result())
                yield TranscriptionResult(
                    text=result['text'],
                    confidence=result.get('conf'),
                    is_final=True
                )
            else:
                partial = json.loads(self._recognizer.PartialResult())
                yield TranscriptionResult(
                    text=partial.get('partial', ''),
                    confidence=None,
                    is_final=False
                )

    def transcribe_file(self, file_path: str) -> Transcription:
        with open(file_path, 'rb') as f:
            audio_data = f.read()
        return self.transcribe(audio_data)

    def get_supported_languages(self) -> Tuple[str, ...]:
        return ('es', 'en')

    def set_language(self, language_code: str) -> None:
        self._validate_language(language_code)
        self._load_model()

    def _create_transcription(self, text: str, confidence: float, duration: float) -> Transcription:
        return self.create_transcription_object(
            text=text,
            confidence=confidence,
            audio_duration=duration
        )

    def _validate_audio_input(self, audio_data: bytes):
        if len(audio_data) < 1024:
            raise ValueError("Audio data too short").
 
=== C:\Users\USUARIO\Documents\ProyectoFinalCA\infrastructure\emotion\pysentimiento_detector.py === 
# infrastructure/emotion/pysentimiento_detector.py
from pysentimiento import create_analyzer
from typing import Dict
import logging
from interfaces.emotion_detection import (
    EmotionDetector,
    EmotionDetectionResult,
    EmotionType,
    EmotionDetectionError
)

class PysentimientoEmotionDetector(EmotionDetector):
    def __init__(self, lang: str = "es"):
        super().__init__()
        self.lang = lang
        self._analyzer = None
        self._load_model()

    def _load_model(self):
        try:
            self._analyzer = create_analyzer(task="emotion", lang=self.lang)
        except Exception as e:
            raise EmotionDetectionError(f"Failed to load pysentimiento model: {str(e)}")

    def analyze_text(self, text: str, language: str = None) -> EmotionDetectionResult:
        language = language or self.lang
        self._validate_text_input(text)
        self.set_language(language)
        
        try:
            result = self._analyzer.predict(text)
            return self._parse_result(result, text)
        except Exception as e:
            logging.error(f"Text analysis failed: {str(e)}")
            raise EmotionDetectionError("Text emotion detection failed") from e

    def analyze_audio(self, audio_data: bytes, sample_rate: int = None) -> EmotionDetectionResult:
        raise NotImplementedError("Pysentimiento detector only supports text analysis")

    def get_supported_emotions(self) -> tuple:
        return (
            EmotionType.ANGRY,
            EmotionType.HAPPY,
            EmotionType.SAD,
            EmotionType.FEAR,
            EmotionType.UNIDENTIFIED
        )

    def get_detector_metadata(self) -> Dict:
        return {
            "detector_type": "text",
            "model_name": "pysentimiento",
            "language": self.lang,
            "version": "0.7.1"
        }

    def _parse_result(self, result, text: str) -> EmotionDetectionResult:
        output = result.output
        probas = result.probas
        
        emotion_map = {
            "anger": EmotionType.ANGRY,
            "joy": EmotionType.HAPPY,
            "sadness": EmotionType.SAD,
            "fear": EmotionType.FEAR,
            "others": EmotionType.UNIDENTIFIED
        }
        
        emotion_probs = {
            emotion_map[k]: v for k, v in probas.items()
        }
        
        predominant = emotion_map[output]
        
        return EmotionDetectionResult(
            predominant_emotion=predominant,
            emotion_probabilities=emotion_probs,
            text_length=len(text),
            confidence=probas[output],
            detector_version=self.get_detector_metadata()["version"]
        )

    def get_detector_type(self) -> str:
        return "text".
 
=== C:\Users\USUARIO\Documents\ProyectoFinalCA\infrastructure\emotion\vokaturi_detector.py === 
# infrastructure/emotion/vokaturi_detector.py
import Vokaturi
import numpy as np
import logging
from typing import Dict
from datetime import datetime
from interfaces.emotion_detection import (
    EmotionDetector,
    EmotionDetectionResult,
    EmotionType,
    EmotionDetectionError,
    InvalidAudioDataError
)

class VokaturiEmotionDetector(EmotionDetector):
    def __init__(self, sample_rate: int = 44100, buffer_length: int = 1024, dll_path: str = ""):
        super().__init__()
        self.sample_rate = sample_rate
        self.buffer_length = buffer_length
        self._voice = None
        self._load_library(dll_path)

    def _load_library(self, dll_path: str):
        try:
            Vokaturi.load(dll_path)
            self._voice = Vokaturi.Voice(
                self.sample_rate,
                self.buffer_length,
                True
            )
        except Exception as e:
            raise EmotionDetectionError(f"Error loading Vokaturi library: {str(e)}")

    def analyze_audio(self, audio_data: bytes, sample_rate: int = None) -> EmotionDetectionResult:
        sample_rate = sample_rate or self.sample_rate
        self._validate_audio_input(audio_data, sample_rate)

        try:
            audio_array = np.frombuffer(audio_data, dtype=np.float32)
            self._voice.fill_float32array(len(audio_array), audio_array)
            
            quality = Vokaturi.Quality()
            probabilities = Vokaturi.EmotionProbabilities()
            self._voice.extract(quality, probabilities)
            
            if not quality.valid:
                raise InvalidAudioDataError("Audio quality insufficient for analysis")
            
            return self._create_result(probabilities, len(audio_data) / sample_rate)
            
        except Exception as e:
            logging.error(f"Vokaturi analysis failed: {str(e)}")
            raise EmotionDetectionError("Emotion detection failed") from e

    def analyze_text(self, text: str, language: str = 'es') -> EmotionDetectionResult:
        raise NotImplementedError("Vokaturi detector only supports audio analysis")

    def get_supported_emotions(self) -> tuple:
        return (
            EmotionType.NEUTRAL,
            EmotionType.HAPPY,
            EmotionType.SAD,
            EmotionType.ANGRY,
            EmotionType.FEAR
        )

    def get_detector_metadata(self) -> Dict:
        return {
            "detector_type": "audio",
            "version": Vokaturi.versionAndLicense(),
            "sample_rate": self.sample_rate,
            "buffer_length": self.buffer_length
        }

    def _create_result(self, probabilities, duration: float) -> EmotionDetectionResult:
        emotion_probs = {
            EmotionType.NEUTRAL: probabilities.neutrality,
            EmotionType.HAPPY: probabilities.happiness,
            EmotionType.SAD: probabilities.sadness,
            EmotionType.ANGRY: probabilities.anger,
            EmotionType.FEAR: probabilities.fear
        }
        
        predominant = max(emotion_probs, key=emotion_probs.get)
        
        return EmotionDetectionResult(
            predominant_emotion=predominant,
            emotion_probabilities=emotion_probs,
            audio_duration=duration,
            confidence=emotion_probs[predominant],
            detector_version=self.get_detector_metadata()["version"]
        )

    def get_detector_type(self) -> str:
        return "audio"

    # **Métodos agregados para corregir el error**
    def get_supported_languages(self) -> tuple:
        """Devuelve los idiomas soportados para análisis de texto"""
        return ("es", "en")  # Agrega los idiomas realmente soportados

    @property
    def supported_emotions(self) -> tuple:
        """Devuelve la lista de emociones soportadas"""
        return (
            EmotionType.NEUTRAL,
            EmotionType.HAPPY,
            EmotionType.SAD,
            EmotionType.ANGRY,
            EmotionType.FEAR
        )
.
 
=== C:\Users\USUARIO\Documents\ProyectoFinalCA\interfaces\__init__.py === 
.
 
=== C:\Users\USUARIO\Documents\ProyectoFinalCA\interfaces\ai\suggestion_generator.py === 
# interfaces/ai/suggestion_generator.py
from abc import ABC, abstractmethod
from dataclasses import dataclass
from enum import Enum
from typing import List, Dict, Optional
import logging
from datetime import datetime
from core.entities.emotion import EmotionType, EmotionAnalysis
from core.entities.transcription import Transcription

class SuggestionGenerationError(Exception):
    """Excepción base para errores de generación de sugerencias"""
    pass

class InvalidInputError(SuggestionGenerationError):
    """Error cuando los parámetros de entrada son inválidos"""
    pass

class ModelOverloadError(SuggestionGenerationError):
    """Error cuando el modelo está sobrecargado"""
    pass

class SuggestionType(Enum):
    STANDARD = "standard"
    PROACTIVE = "proactive"
    MITIGATION = "mitigation"
    ESCALATION = "escalation"

@dataclass(frozen=True)
class Suggestion:
    text: str
    suggestion_type: SuggestionType
    target_emotion: EmotionType
    confidence: float
    generation_date: datetime = datetime.now()
    metadata: Optional[Dict] = None

    def __post_init__(self):
        if not 0 <= self.confidence <= 1:
            raise ValueError("La confianza debe estar entre 0 y 1")
        
        if len(self.text.strip()) < 10:
            raise ValueError("El texto de sugerencia es demasiado corto")

class SuggestionGenerator(ABC):
    def __init__(self):
        self.logger = logging.getLogger(self.__class__.__name__)
        self._max_suggestions = 5
        self._min_confidence = 0.5

    @abstractmethod
    def generate_from_emotion(
        self,
        emotion_analysis: EmotionAnalysis,
        context: Optional[Dict] = None
    ) -> List[Suggestion]:
        """
        Genera sugerencias basadas en análisis de emociones
        Args:
            emotion_analysis: Resultado del análisis de emociones
            context: Contexto adicional para la generación
        Returns:
            Lista de sugerencias ordenadas por relevancia
        """
        pass

    @abstractmethod
    def generate_from_text(
        self,
        text: str,
        language: str = "es",
        context: Optional[Dict] = None
    ) -> List[Suggestion]:
        """
        Genera sugerencias basadas en texto de entrada
        Args:
            text: Texto a analizar
            language: Idioma del texto (código ISO 639-1)
            context: Contexto adicional para la generación
        Returns:
            Lista de sugerencias ordenadas por relevancia
        """
        pass

    @abstractmethod
    def generate_from_transcription(
        self,
        transcription: Transcription
    ) -> List[Suggestion]:
        """
        Genera sugerencias basadas en una transcripción completa
        Args:
            transcription: Objeto Transcription con metadatos
        Returns:
            Lista de sugerencias contextuales
        """
        pass

    @abstractmethod
    def get_supported_languages(self) -> List[str]:
        """Devuelve lista de idiomas soportados"""
        pass

    @abstractmethod
    def model_version(self) -> str:
        """Devuelve versión del modelo utilizado"""
        pass

    @abstractmethod
    def get_max_suggestions(self) -> int:
        """Devuelve el número máximo de sugerencias a generar"""
        pass

    @abstractmethod
    def set_generation_parameters(
        self,
        temperature: float = 0.7,
        max_length: int = 100,
        creativity: float = 0.5
    ) -> None:
        """Configura parámetros de generación del modelo"""
        pass

    def _validate_inputs(
        self,
        text: Optional[str] = None,
        emotion_analysis: Optional[EmotionAnalysis] = None
    ) -> None:
        """Valida los parámetros de entrada comunes"""
        if text and len(text.strip()) < 5:
            raise InvalidInputError("El texto de entrada es demasiado corto")
            
        if emotion_analysis and not isinstance(emotion_analysis, EmotionAnalysis):
            raise InvalidInputError("Análisis de emociones inválido")

    def _apply_business_rules(
        self,
        suggestions: List[Suggestion]
    ) -> List[Suggestion]:
        """Aplica reglas de negocio a las sugerencias generadas"""
        return sorted(
            [s for s in suggestions if s.confidence >= self._min_confidence],
            key=lambda x: (-x.confidence, x.suggestion_type.value)
        )[:self._max_suggestions]

    @abstractmethod
    def get_performance_metrics(self) -> Dict:
        """Obtiene métricas de rendimiento del generador"""
        pass.
 
=== C:\Users\USUARIO\Documents\ProyectoFinalCA\interfaces\ai\__init__.py === 
from .suggestion_generator import SuggestionGenerator, SuggestionType, SuggestionGenerationError, ModelOverloadError, InvalidInputError.
 
=== C:\Users\USUARIO\Documents\ProyectoFinalCA\interfaces\audio\recorder.py === 
# interfaces/audio/recorder.py
from abc import ABC, abstractmethod
from typing import Optional, Generator, Union, Tuple
from dataclasses import dataclass
import logging

class RecordingError(Exception):
    """Excepción base para errores de grabación de audio"""
    pass

class DeviceConfigurationError(RecordingError):
    """Error en configuración de dispositivos de audio"""
    pass

@dataclass(frozen=True)
class AudioFormat:
    sample_rate: int = 44100
    channels: int = 1
    format: str = 'float32'
    chunk_size: int = 1024

class AudioRecorder(ABC):
    def __init__(self, format: AudioFormat = AudioFormat()):
        self.format = format
        self._is_recording = False
        self.logger = logging.getLogger(self.__class__.__name__)

    @abstractmethod
    def start_recording(self) -> None:
        """Inicia la grabación de audio"""
        pass

    @abstractmethod
    def stop_recording(self) -> None:
        """Detiene la grabación de audio"""
        pass

    @property
    @abstractmethod
    def is_recording(self) -> bool:
        """Indica si la grabación está en curso"""
        pass

    @abstractmethod
    def get_audio_stream(self) -> Generator[bytes, None, None]:
        """Generador que produce chunks de audio en tiempo real"""
        pass

    @abstractmethod
    def get_full_recording(self) -> bytes:
        """Obtiene la grabación completa como un solo buffer"""
        pass

    @abstractmethod
    def list_devices(self) -> Tuple[int, str]:
        """Lista los dispositivos de audio disponibles"""
        pass

    @abstractmethod
    def configure_device(self, device_index: int) -> None:
        """Configura el dispositivo de grabación"""
        pass

    def record_to_file(self, file_path: str) -> None:
        """Grabación directa a archivo (implementación por defecto)"""
        try:
            self.start_recording()
            with open(file_path, 'wb') as f:
                for chunk in self.get_audio_stream():
                    f.write(chunk)
        except Exception as e:
            self.logger.error(f"Error grabando a archivo: {str(e)}")
            raise RecordingError("Error en grabación a archivo") from e
        finally:
            self.stop_recording().
 
=== C:\Users\USUARIO\Documents\ProyectoFinalCA\interfaces\audio\speech_to_text.py === 
# interfaces/audio/speech_to_text.py
from abc import ABC, abstractmethod
from typing import Optional, Union, Generator, Tuple
from dataclasses import dataclass
import logging
from core.entities.transcription import Transcription

class TranscriptionError(Exception):
    """Excepción base para errores de transcripción"""
    pass

class LanguageNotSupportedError(TranscriptionError):
    """Error cuando el idioma no está soportado"""
    pass

@dataclass(frozen=True)
class TranscriptionConfig:
    language: str = 'es-ES'
    interim_results: bool = True
    max_alternatives: int = 1
    word_confidence: bool = False
    speaker_diarization: bool = False
    sample_rate: int = 16000  # Añade este atributo

@dataclass(frozen=True)
class TranscriptionResult:
    text: str
    confidence: Optional[float]
    language: str
    is_final: bool = True
    alternatives: Tuple[str, ...] = ()

class SpeechToText(ABC):
    def __init__(self, config: TranscriptionConfig = TranscriptionConfig()):
        self.config = config
        self.logger = logging.getLogger(self.__class__.__name__)

    @abstractmethod
    def transcribe(self, audio_data: bytes) -> Transcription:
        """Transcribe audio completo a texto"""
        pass

    @abstractmethod
    def transcribe_streaming(
        self, 
        audio_stream: Generator[bytes, None, None]
    ) -> Generator[TranscriptionResult, None, None]:
        """Transcribe un stream de audio en tiempo real"""
        pass

    @abstractmethod
    def transcribe_file(self, file_path: str) -> Transcription:
        """Transcribe un archivo de audio directamente"""
        pass

    @abstractmethod
    def get_supported_languages(self) -> Tuple[str, ...]:
        """Obtiene lista de idiomas soportados"""
        pass

    @abstractmethod
    def set_language(self, language_code: str) -> None:
        """Configura el idioma para transcripción"""
        pass

    def _validate_language(self, language_code: str) -> bool:
        """Valida que el idioma esté soportado"""
        supported = self.get_supported_languages()
        if language_code not in supported:
            raise LanguageNotSupportedError(
                f"Idioma {language_code} no soportado. Idiomas disponibles: {', '.join(supported)}"
            )
        return True

    def create_transcription_object(
        self,
        text: str,
        confidence: Optional[float],
        audio_duration: float
    ) -> Transcription:
        """Factory method para crear objetos Transcription"""
        return Transcription(
            text=text,
            confidence=confidence,
            audio_duration=audio_duration,
            audio_source=self.__class__.__name__
        ).
 
=== C:\Users\USUARIO\Documents\ProyectoFinalCA\interfaces\audio\__init__.py === 
from .speech_to_text import SpeechToText.
 
=== C:\Users\USUARIO\Documents\ProyectoFinalCA\interfaces\emotion_detection\emotion_detector.py === 
# interfaces/emotion_detection/emotion_detector.py
from abc import ABC, abstractmethod
from dataclasses import dataclass
from enum import Enum
from typing import Tuple, Optional, Dict
import logging

class EmotionDetectionError(Exception):
    """Excepción base para errores de detección de emociones"""
    pass

class InvalidAudioDataError(EmotionDetectionError):
    """Error cuando los datos de audio son inválidos"""
    pass

class UnsupportedLanguageError(EmotionDetectionError):
    """Error cuando el idioma no está soportado"""
    pass

class EmotionType(Enum):
    NEUTRAL = "neutral"
    HAPPY = "happy"
    SAD = "sad"
    ANGRY = "angry"
    FEAR = "fear"
    UNIDENTIFIED = "unidentified"

@dataclass(frozen=True)
class EmotionDetectionResult:
    predominant_emotion: EmotionType
    emotion_probabilities: Dict[EmotionType, float]
    audio_duration: Optional[float] = None
    text_length: Optional[int] = None
    confidence: Optional[float] = None
    detector_version: str = "1.0"

    def __post_init__(self):
        if sum(self.emotion_probabilities.values()) < 0.99:
            raise ValueError("Las probabilidades deben sumar aproximadamente 1")
            
        if self.confidence and not (0 <= self.confidence <= 1):
            raise ValueError("La confianza debe estar entre 0 y 1")

class EmotionDetector(ABC):
    def __init__(self):
        self.logger = logging.getLogger(self.__class__.__name__)
        self._supported_languages = ['es', 'en']
        self._current_language = 'es'

    @abstractmethod
    def analyze_audio(self, audio_data: bytes, sample_rate: int = 44100) -> EmotionDetectionResult:
        """
        Analiza emociones a partir de datos de audio crudos
        Args:
            audio_data: Bytes del audio a analizar
            sample_rate: Tasa de muestreo del audio
        Returns:
            EmotionDetectionResult: Resultado del análisis
        """
        pass

    @abstractmethod
    def analyze_text(self, text: str, language: str = 'es') -> EmotionDetectionResult:
        """
        Analiza emociones a partir de texto
        Args:
            text: Texto a analizar
            language: Idioma del texto (código ISO 639-1)
        Returns:
            EmotionDetectionResult: Resultado del análisis
        """
        pass

    @property
    @abstractmethod
    def supported_emotions(self) -> Tuple[EmotionType, ...]:
        """Devuelve la lista de emociones soportadas por el detector"""
        pass

    @abstractmethod
    def get_supported_languages(self) -> Tuple[str, ...]:
        """Devuelve los idiomas soportados para análisis de texto"""
        pass

    def set_language(self, language_code: str) -> None:
        """Configura el idioma para análisis de texto"""
        if language_code not in self._supported_languages:
            raise UnsupportedLanguageError(
                f"Idioma {language_code} no soportado. Idiomas disponibles: {', '.join(self._supported_languages)}"
            )
        self._current_language = language_code
        self.logger.info(f"Idioma configurado a: {language_code}")

    @abstractmethod
    def get_detector_metadata(self) -> Dict:
        """Obtiene metadatos sobre la implementación del detector"""
        pass

    def _validate_audio_input(self, audio_data: bytes, sample_rate: int) -> None:
        """Valida los parámetros de entrada de audio"""
        if not audio_data:
            raise InvalidAudioDataError("Los datos de audio no pueden estar vacíos")
        
        if sample_rate < 8000 or sample_rate > 48000:
            raise InvalidAudioDataError("Tasa de muestreo no soportada")

    def _validate_text_input(self, text: str) -> None:
        """Valida los parámetros de entrada de texto"""
        if len(text.strip()) < 10:
            raise ValueError("El texto debe contener al menos 10 caracteres")

    @abstractmethod
    def get_detector_type(self) -> str:
        """Devuelve el tipo de detector (audio/texto/multimodal)"""
        pass.
 
=== C:\Users\USUARIO\Documents\ProyectoFinalCA\interfaces\emotion_detection\__init__.py === 
from .emotion_detector import EmotionDetector, EmotionType, EmotionDetectionResult, EmotionDetectionError, InvalidAudioDataError, UnsupportedLanguageError.
 
=== C:\Users\USUARIO\Documents\ProyectoFinalCA\presentation\dashboard\index.html === 
<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Dashboard de Análisis de Emociones</title>
    
    <!-- CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/bootstrap-icons/1.8.1/font/bootstrap-icons.min.css">
    
    <style>
        .emotion-card {
            transition: transform 0.3s;
            cursor: pointer;
        }
        .emotion-card:hover {
            transform: translateY(-5px);
        }
        #livePulse {
            width: 20px;
            height: 20px;
            border-radius: 50%;
            animation: pulse 2s infinite;
        }
        @keyframes pulse {
            0% { box-shadow: 0 0 0 0 rgba(0, 150, 255, 0.7); }
            70% { box-shadow: 0 0 0 15px rgba(0, 150, 255, 0); }
            100% { box-shadow: 0 0 0 0 rgba(0, 150, 255, 0); }
        }
    </style>
</head>
<body class="bg-light">
    <div class="container-fluid p-4">
        <div class="row mb-4">
            <div class="col">
                <h1 class="display-4">
                    <i class="bi bi-graph-up"></i> 
                    Análisis de Emociones en Tiempo Real
                    <span id="livePulse" class="ms-2"></span>
                </h1>
            </div>
        </div>

        <!-- Controles -->
        <div class="row mb-4">
            <div class="col-md-4">
                <div class="card shadow">
                    <div class="card-body">
                        <h5 class="card-title">
                            <i class="bi bi-mic"></i> Grabación en Vivo
                        </h5>
                        <button id="startRecording" class="btn btn-success">
                            <i class="bi bi-record-circle"></i> Iniciar
                        </button>
                        <button id="stopRecording" class="btn btn-danger" disabled>
                            <i class="bi bi-stop-circle"></i> Detener
                        </button>
                    </div>
                </div>
            </div>
            
            <div class="col-md-8">
                <div class="card shadow">
                    <div class="card-body">
                        <h5 class="card-title">
                            <i class="bi bi-upload"></i> Subir Audio
                        </h5>
                        <input type="file" id="audioFile" accept=".wav,.mp3" class="form-control">
                    </div>
                </div>
            </div>
        </div>

        <!-- Estadísticas -->
        <div class="row mb-4">
            <div class="col-md-4">
                <div class="card shadow emotion-card bg-info text-white">
                    <div class="card-body">
                        <h5><i class="bi bi-emoji-neutral"></i> Neutral</h5>
                        <h2 id="neutralPercent">0%</h2>
                        <div class="progress bg-white">
                            <div id="neutralProgress" class="progress-bar" role="progressbar"></div>
                        </div>
                    </div>
                </div>
            </div>
            
            <div class="col-md-4">
                <div class="card shadow emotion-card bg-warning">
                    <div class="card-body">
                        <h5><i class="bi bi-emoji-smile"></i> Feliz</h5>
                        <h2 id="happyPercent">0%</h2>
                        <div class="progress bg-white">
                            <div id="happyProgress" class="progress-bar" role="progressbar"></div>
                        </div>
                    </div>
                </div>
            </div>
            
            <div class="col-md-4">
                <div class="card shadow emotion-card bg-danger text-white">
                    <div class="card-body">
                        <h5><i class="bi bi-emoji-angry"></i> Enojado</h5>
                        <h2 id="angryPercent">0%</h2>
                        <div class="progress bg-white">
                            <div id="angryProgress" class="progress-bar" role="progressbar"></div>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <!-- Gráficos y Sugerencias -->
        <div class="row">
            <div class="col-md-8">
                <div class="card shadow mb-4">
                    <div class="card-body">
                        <h5 class="card-title">
                            <i class="bi bi-bar-chart-line"></i> Distribución de Emociones
                        </h5>
                        <canvas id="emotionChart"></canvas>
                    </div>
                </div>
                
                <div class="card shadow">
                    <div class="card-body">
                        <h5 class="card-title">
                            <i class="bi bi-chat-left-text"></i> Transcripción en Tiempo Real
                        </h5>
                        <div id="transcriptionBox" class="bg-light p-3 rounded" style="height: 200px; overflow-y: auto;">
                            <!-- Transcripciones aparecerán aquí -->
                        </div>
                    </div>
                </div>
            </div>

            <div class="col-md-4">
                <div class="card shadow">
                    <div class="card-body">
                        <h5 class="card-title">
                            <i class="bi bi-lightbulb"></i> Sugerencias
                        </h5>
                        <div id="suggestionsList" class="list-group">
                            <!-- Sugerencias aparecerán aquí -->
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <!-- Scripts -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <script src="assets/js/archivo.js"></script>
    
    <script>
        // Configuración inicial del gráfico
        const ctx = document.getElementById('emotionChart').getContext('2d');
        const emotionChart = new Chart(ctx, {
            type: 'doughnut',
            data: {
                labels: ['Neutral', 'Feliz', 'Enojado', 'No Identificado'],
                datasets: [{
                    data: [0, 0, 0, 0],
                    backgroundColor: [
                        '#0dcaf0',
                        '#ffc107',
                        '#dc3545',
                        '#6c757d'
                    ]
                }]
            },
            options: {
                responsive: true,
                plugins: {
                    legend: { position: 'bottom' }
                }
            }
        });

        // WebSocket para actualizaciones en tiempo real
        const socket = io.connect('http://localhost:5000');

            socket.on('update', function(data) {
                updateDashboard(data);
            });

        // Manejo de grabación de audio
        let mediaRecorder;
        let audioChunks = [];
        
        document.getElementById('startRecording').addEventListener('click', async () => {
            try {
                const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                mediaRecorder = new MediaRecorder(stream);
                
                mediaRecorder.ondataavailable = event => {
                    audioChunks.push(event.data);
                    ws.send(event.data);
                };
                
                mediaRecorder.start(1000);
                
                document.getElementById('startRecording').disabled = true;
                document.getElementById('stopRecording').disabled = false;
            } catch (err) {
                console.error('Error al acceder al micrófono:', err);
            }
        });

        document.getElementById('stopRecording').addEventListener('click', () => {
            mediaRecorder.stop();
            document.getElementById('startRecording').disabled = false;
            document.getElementById('stopRecording').disabled = true;
        });

        // Actualización del dashboard
        function updateDashboard(data) {
            // Actualizar gráfico
            emotionChart.data.datasets[0].data = [
                data.emotion_analysis.neutrality * 100,
                data.emotion_analysis.happiness * 100,
                data.emotion_analysis.anger * 100,
                data.emotion_analysis.unidentified * 100
            ];
            emotionChart.update();

            // Actualizar tarjetas
            document.getElementById('neutralPercent').textContent = 
                `${Math.round(data.emotion_analysis.neutrality * 100)}%`;
            document.getElementById('neutralProgress').style.width = 
                `${data.emotion_analysis.neutrality * 100}%`;

            document.getElementById('happyPercent').textContent = 
                `${Math.round(data.emotion_analysis.happiness * 100)}%`;
            document.getElementById('happyProgress').style.width = 
                `${data.emotion_analysis.happiness * 100}%`;

            document.getElementById('angryPercent').textContent = 
                `${Math.round(data.emotion_analysis.anger * 100)}%`;
            document.getElementById('angryProgress').style.width = 
                `${data.emotion_analysis.anger * 100}%`;

            // Actualizar transcripción
            const transcriptionBox = document.getElementById('transcriptionBox');
            transcriptionBox.innerHTML += `<div class="mb-2">${data.partial_text}</div>`;
            transcriptionBox.scrollTop = transcriptionBox.scrollHeight;

            // Actualizar sugerencias
            const suggestionsList = document.getElementById('suggestionsList');
            suggestionsList.innerHTML = data.suggestions
                .map(sug => `
                    <a href="#" class="list-group-item list-group-item-action">
                        <i class="bi bi-chat-square-text"></i>
                        ${sug.text}
                        <span class="badge bg-${getBadgeColor(sug.suggestion_type)} float-end">
                            ${sug.suggestion_type}
                        </span>
                    </a>
                `).join('');
        }

        function getBadgeColor(type) {
            const colors = {
                standard: 'primary',
                proactive: 'success',
                mitigation: 'warning',
                escalation: 'danger'
            };
            return colors[type.toLowerCase()] || 'secondary';
        }

        // Manejo de subida de archivos
        document.getElementById('audioFile').addEventListener('change', async (e) => {
            const file = e.target.files[0];
            const formData = new FormData();
            formData.append('audio', file);

            try {
                const response = await fetch('/api/analyze/audio', {
                    method: 'POST',
                    body: formData
                });
                
                const data = await response.json();
                updateDashboard(data);
            } catch (error) {
                console.error('Error al subir archivo:', error);
            }
        });
    </script>
</body>
</html>.
 
=== C:\Users\USUARIO\Documents\ProyectoFinalCA\presentation\web\app.py === 
# presentation/web/app.py
from flask import Flask
from flask_cors import CORS
from flask_socketio import SocketIO 
from infrastructure.audio.vosk_stt import VoskSpeechToText
from infrastructure.audio.vokaturi_recorder import VokaturiRecorder
from infrastructure.emotion.vokaturi_detector import VokaturiEmotionDetector
from infrastructure.ai.openai_generator import OpenAIGenerator
from core.use_cases import (
    AudioProcessingUseCase,
    EmotionAnalysisUseCase,
    SuggestionGenerationUseCase
)
import os

def create_app(config=None):
    app = Flask(__name__)
    CORS(app, resources={r"/*": {"origins": "*"}})
    socketio = SocketIO(app, cors_allowed_origins="*")
    
    # Configuración
    app.config.from_mapping(
        VOKATURI_DLL_PATH=os.getenv('VOKATURI_DLL', 'libs/OpenVokaturi-4-0-win64.dll'),
        OPENAI_API_KEY=os.getenv('OPENAI_API_KEY'),
        AUDIO_SAMPLE_RATE=16000,
        MAX_CONTENT_LENGTH=16 * 1024 * 1024  # 16MB
    )

    # Inicialización de dependencias
    with app.app_context():
        # Infraestructura
        stt_service = VoskSpeechToText()
        recorder = VokaturiRecorder(dll_path=app.config['VOKATURI_DLL_PATH'])
        emotion_detector = VokaturiEmotionDetector(
            dll_path=app.config['VOKATURI_DLL_PATH']  # <-- Añadir este parámetro
            )
        suggestion_generators = [
            OpenAIGenerator(api_key=app.config['OPENAI_API_KEY'])
        ]

        # Casos de uso
        audio_processor = AudioProcessingUseCase(stt_service)
        emotion_analyzer = EmotionAnalysisUseCase(emotion_detector)
        suggestion_generator = SuggestionGenerationUseCase(suggestion_generators)

        # Registrar rutas
        from .routes import configure_routes
        configure_routes(socketio, audio_processor, emotion_analyzer, suggestion_generator)

    return socketio

if __name__ == "__main__":
    socketio = create_app()
    socketio.run(host='0.0.0.0', port=5000, debug=True, use_reloader=False).
 
=== C:\Users\USUARIO\Documents\ProyectoFinalCA\presentation\web\routes.py === 
# presentation/web/routes.py
from flask import jsonify, request, session
from werkzeug.exceptions import BadRequest, InternalServerError
from flask_socketio import emit  # Nuevo import
import logging
import time
import uuid
import json

def configure_routes(socketio, audio_processor, emotion_analyzer, suggestion_generator):
    app = socketio.server.app  # Acceder a la app Flask subyacente

    @app.route('/api/health', methods=['GET'])
    def health_check():
        return jsonify({
            "status": "ok",
            "timestamp": time.time(),
            "services": {
                "audio_processing": "active",
                "emotion_analysis": "active",
                "suggestion_generation": "active"
            }
        })

    @app.route('/api/analyze/audio', methods=['POST'])
    def analyze_audio():
        try:
            if 'audio' not in request.files:
                raise BadRequest("No audio file provided")
            
            audio_file = request.files['audio']
            session_id = request.form.get('session_id', str(uuid.uuid4()))
            
            audio_data = audio_file.read()
            transcription = audio_processor.process_audio(audio_data)
            emotion_analysis = emotion_analyzer.analyze_audio(audio_data)
            
            suggestions = suggestion_generator.generate_from_emotion(
                emotion_analysis,
                context={
                    "session_id": session_id,
                    "text": transcription.text
                }
            )
            
            return jsonify({
                "session_id": session_id,
                "transcription": transcription.to_dict(),
                "emotion_analysis": emotion_analysis.to_dict(),
                "suggestions": [s.to_dict() for s in suggestions]
            })
            
        except Exception as e:
            logging.error(f"Audio analysis error: {str(e)}")
            raise InternalServerError("Error processing audio")

    @app.route('/api/analyze/text', methods=['POST'])
    def analyze_text():
        try:
            data = request.get_json()
            if not data or 'text' not in data:
                raise BadRequest("Missing text in request body")
            
            emotion_analysis = emotion_analyzer.analyze_text(data['text'])
            suggestions = suggestion_generator.generate_from_text(
                data['text'],
                context=data.get('context', {})
            )
            
            return jsonify({
                "text": data['text'],
                "emotion_analysis": emotion_analysis.to_dict(),
                "suggestions": [s.to_dict() for s in suggestions]
            })
            
        except Exception as e:
            logging.error(f"Text analysis error: {str(e)}")
            raise InternalServerError("Error processing text")

    # WebSocket implementation con SocketIO
    @socketio.on('real_time_audio')  # Usa la instancia socketio
    def handle_real_time_stream(data):
        session_id = str(uuid.uuid4())
        buffer = bytearray(data['audio_chunk'])
        
        try:
            if len(buffer) >= 4096:
                chunk = bytes(buffer[:4096])
                transcription = audio_processor.process_audio(chunk)
                emotion_analysis = emotion_analyzer.analyze_audio(chunk)
                
                emit('audio_update', {
                    "session_id": session_id,
                    "partial_text": transcription.text,
                    "emotion": emotion_analysis.predominant_emotion.value,
                    "confidence": emotion_analysis.confidence
                })
                
        except Exception as e:
            logging.error(f"WebSocket error: {str(e)}")
            emit('error', {'message': str(e)})

    @socketio.on('finalize_audio')  # Usa la instancia socketio
    def handle_final_audio(data):
        try:
            final_transcription = audio_processor.process_audio(data['final_audio'])
            emotion_analysis = emotion_analyzer.analyze_audio(data['final_audio'])
            suggestions = suggestion_generator.generate_from_transcription(final_transcription)
            
            emit('final_result', {
                "session_id": data['session_id'],
                "final_text": final_transcription.text,
                "emotion_analysis": emotion_analysis.to_dict(),
                "suggestions": [s.to_dict() for s in suggestions]
            })
            
        except Exception as e:
            logging.error(f"Final processing error: {str(e)}")

    # Resto del código sin cambios...
    @app.errorhandler(400)
    def handle_bad_request(e):
        return jsonify({
            "error": "Bad Request",
            "message": str(e.description)
        }), 400

    @app.errorhandler(500)
    def handle_server_error(e):
        return jsonify({
            "error": "Internal Server Error",
            "message": "An unexpected error occurred"
        }), 500

    @app.route('/api/history/<session_id>', methods=['GET'])
    def get_session_history(session_id):
        return jsonify({
            "session_id": session_id,
            "data": "Historial no implementado"
        })

    @app.route('/api/feedback', methods=['POST'])
    def save_feedback():
        try:
            data = request.get_json()
            return jsonify({"status": "Feedback recibido"})
        except Exception as e:
            logging.error(f"Feedback error: {str(e)}")
            raise InternalServerError("Error saving feedback").
